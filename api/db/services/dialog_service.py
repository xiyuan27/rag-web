#
#  Copyright 2024 The InfiniFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
import binascii
import logging
import re
import time
from copy import deepcopy
from datetime import datetime
from functools import partial
from timeit import default_timer as timer

from langfuse import Langfuse

from agentic_reasoning import DeepResearcher
from api import settings
from api.db import LLMType, ParserType, StatusEnum
from api.db.db_models import DB, Dialog
from api.db.services.common_service import CommonService
from api.db.services.knowledgebase_service import KnowledgebaseService
from api.db.services.langfuse_service import TenantLangfuseService
from api.db.services.llm_service import LLMBundle, TenantLLMService
from api.utils import current_timestamp, datetime_format
from rag.app.resume import forbidden_select_fields4resume
from rag.app.tag import label_question
from rag.nlp.search import index_name
from rag.prompts import chunks_format, citation_prompt, cross_languages, full_question, kb_prompt, keyword_extraction, \
    llm_id2llm_type, message_fit_in
from rag.utils import num_tokens_from_string, rmSpace
from rag.utils.tavily_conn import Tavily


class DialogService(CommonService):
    model = Dialog

    @classmethod
    def save(cls, **kwargs):
        """Save a new record to database.

        This method creates a new record in the database with the provided field values,
        forcing an insert operation rather than an update.

        Args:
            **kwargs: Record field values as keyword arguments.

        Returns:
            Model instance: The created record object.
        """
        sample_obj = cls.model(**kwargs).save(force_insert=True)
        return sample_obj

    @classmethod
    def update_many_by_id(cls, data_list):
        """Update multiple records by their IDs.

        This method updates multiple records in the database, identified by their IDs.
        It automatically updates the update_time and update_date fields for each record.

        Args:
            data_list (list): List of dictionaries containing record data to update.
                             Each dictionary must include an 'id' field.
        """
        with DB.atomic():
            for data in data_list:
                data["update_time"] = current_timestamp()
                data["update_date"] = datetime_format(datetime.now())
                cls.model.update(data).where(cls.model.id == data["id"]).execute()

    @classmethod
    @DB.connection_context()
    def get_list(cls, tenant_id, page_number, items_per_page, orderby, desc, id, name):
        chats = cls.model.select()
        if id:
            chats = chats.where(cls.model.id == id)
        if name:
            chats = chats.where(cls.model.name == name)
        chats = chats.where((cls.model.tenant_id == tenant_id) & (cls.model.status == StatusEnum.VALID.value))
        if desc:
            chats = chats.order_by(cls.model.getter_by(orderby).desc())
        else:
            chats = chats.order_by(cls.model.getter_by(orderby).asc())

        chats = chats.paginate(page_number, items_per_page)

        return list(chats.dicts())


def chat_solo(dialog, messages, stream=True):
    """
       çº¯çº¯èŠå¤©æ¨¡å¼ï¼ˆä¸ä½¿ç”¨çŸ¥è¯†åº“æ£€ç´¢ï¼‰
    """
    print("çº¯çº¯èŠå¤©æ¨¡å¼ï¼ˆä¸ä½¿ç”¨çŸ¥è¯†åº“æ£€ç´¢ï¼‰")
    if llm_id2llm_type(dialog.llm_id) == "image2text":
        chat_mdl = LLMBundle(dialog.tenant_id, LLMType.IMAGE2TEXT, dialog.llm_id)
    else:
        chat_mdl = LLMBundle(dialog.tenant_id, LLMType.CHAT, dialog.llm_id)

    prompt_config = dialog.prompt_config
    tts_mdl = None
    if prompt_config.get("tts"):
        tts_mdl = LLMBundle(dialog.tenant_id, LLMType.TTS)
    msg = [{"role": m["role"], "content": re.sub(r"##\d+\$\$", "", m["content"])} for m in messages if
           m["role"] != "system"]
    if stream:
        last_ans = ""
        delta_ans = ""
        print("chat_solo--è°ƒç”¨å¤§æ¨¡å‹ï¼Œæµå¼é€æ­¥ç”Ÿæˆç­”æ¡ˆ")
        for ans in chat_mdl.chat_streamly(prompt_config.get("system", ""), msg, dialog.llm_setting):
            answer = ans
            delta_ans = ans[len(last_ans):]
            if num_tokens_from_string(delta_ans) < 16:
                continue
            last_ans = answer
            yield {"answer": answer, "reference": {}, "audio_binary": tts(tts_mdl, delta_ans), "prompt": "",
                   "created_at": time.time()}
            delta_ans = ""
        if delta_ans:
            yield {"answer": answer, "reference": {}, "audio_binary": tts(tts_mdl, delta_ans), "prompt": "",
                   "created_at": time.time()}
    else:
        answer = chat_mdl.chat(prompt_config.get("system", ""), msg, dialog.llm_setting)
        user_content = msg[-1].get("content", "[content not available]")
        logging.debug("User: {}|Assistant: {}".format(user_content, answer))
        yield {"answer": answer, "reference": {}, "audio_binary": tts(tts_mdl, answer), "prompt": "",
               "created_at": time.time()}


def get_models(dialog):
    print("æ ¹æ®å¯¹è¯é…ç½®åˆå§‹åŒ–æ‰€éœ€çš„æ¨¡å‹å®ä¾‹")
    embd_mdl, chat_mdl, rerank_mdl, tts_mdl = None, None, None, None
    kbs = KnowledgebaseService.get_by_ids(dialog.kb_ids)
    embedding_list = list(set([kb.embd_id for kb in kbs]))
    if len(embedding_list) > 1:
        raise Exception("**ERROR**: Knowledge bases use different embedding models.")

    if embedding_list:
        embd_mdl = LLMBundle(dialog.tenant_id, LLMType.EMBEDDING, embedding_list[0])
        if not embd_mdl:
            raise LookupError("Embedding model(%s) not found" % embedding_list[0])

    if llm_id2llm_type(dialog.llm_id) == "image2text":
        chat_mdl = LLMBundle(dialog.tenant_id, LLMType.IMAGE2TEXT, dialog.llm_id)
    else:
        chat_mdl = LLMBundle(dialog.tenant_id, LLMType.CHAT, dialog.llm_id)

    if dialog.rerank_id:
        rerank_mdl = LLMBundle(dialog.tenant_id, LLMType.RERANK, dialog.rerank_id)

    if dialog.prompt_config.get("tts"):
        tts_mdl = LLMBundle(dialog.tenant_id, LLMType.TTS)
    return kbs, embd_mdl, rerank_mdl, chat_mdl, tts_mdl


BAD_CITATION_PATTERNS = [
    re.compile(r"\(\s*ID\s*[: ]*\s*(\d+)\s*\)"),  # (ID: 12)
    re.compile(r"\[\s*ID\s*[: ]*\s*(\d+)\s*\]"),  # [ID: 12]
    re.compile(r"ã€\s*ID\s*[: ]*\s*(\d+)\s*ã€‘"),  # ã€ID: 12ã€‘
    re.compile(r"ref\s*(\d+)", flags=re.IGNORECASE),  # ref12ã€REF 12
]


def repair_bad_citation_formats(answer: str, kbinfos: dict, idx: set):
    max_index = len(kbinfos["chunks"])

    def safe_add(i):
        if 0 <= i < max_index:
            idx.add(i)
            return True
        return False

    def find_and_replace(pattern, group_index=1, repl=lambda i: f"ID:{i}", flags=0):
        nonlocal answer

        def replacement(match):
            try:
                i = int(match.group(group_index))
                if safe_add(i):
                    return f"[{repl(i)}]"
            except Exception:
                pass
            return match.group(0)

        answer = re.sub(pattern, replacement, answer, flags=flags)

    for pattern in BAD_CITATION_PATTERNS:
        find_and_replace(pattern)

    return answer, idx


def chat(dialog, messages, stream=True, **kwargs):
    """
       å®ç°å®Œæ•´çš„RAGå¯¹è¯æµç¨‹
       Args:
           dialog: æ™ºèƒ½åŠ©æ‰‹é…ç½®ä¿¡æ¯
           messages: æ¶ˆæ¯å†å²åˆ—è¡¨
           stream: æ˜¯å¦æµå¼è¾“å‡º
           **kwargs: å…¶ä»–å‚æ•°
       """
    logging.info("=" * 80)
    logging.info(" RAGå¯¹è¯æµç¨‹å¼€å§‹")
    logging.info("=" * 80)

    logging.info("\n [é˜¶æ®µ1] å¼€å§‹ - åˆå§‹åŒ–å’ŒéªŒè¯")
    assert messages[-1]["role"] == "user", "The last content of this conversation is not from user."
    logging.info(f" éªŒè¯é€šè¿‡ - æœ€åä¸€æ¡æ¶ˆæ¯æ¥è‡ªç”¨æˆ·: {messages[-1]['content'][:50]}...")

    # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ä¸”æ²¡æœ‰Tavily APIå¯†é’¥ï¼Œåˆ™ä½¿ç”¨ç®€å•å¯¹è¯æ¨¡å¼
    if not dialog.kb_ids and not dialog.prompt_config.get("tavily_api_key"):
        logging.info(" ä½¿ç”¨ç®€å•å¯¹è¯æ¨¡å¼ï¼ˆå› ä¸ºæ— çŸ¥è¯†åº“ï¼Œä¸”æ— è”ç½‘æœç´¢-tavily_api_keyä¸ºç©º.ï¼‰")
        for ans in chat_solo(dialog, messages, stream):
            yield ans
        return
    logging.info(" [é˜¶æ®µ1] å®Œæˆ - åˆå§‹åŒ–å’ŒéªŒè¯")

    chat_start_ts = timer()  # å¯¹è¯å¼€å§‹æ—¶é—´

    # ===== é˜¶æ®µ2: æ¨¡å‹é…ç½®è·å– =====
    logging.info("[é˜¶æ®µ2] å¼€å§‹ - æ¨¡å‹é…ç½®è·å–")
    # æ ¹æ®LLMç±»å‹è·å–æ¨¡å‹é…ç½®(å›¾ç‰‡è§£è¯»æ¨¡å‹/èŠå¤©æ¨¡å‹)
    if llm_id2llm_type(dialog.llm_id) == "image2text":
        llm_model_config = TenantLLMService.get_model_config(dialog.tenant_id, LLMType.IMAGE2TEXT, dialog.llm_id)
        logging.info(f"ğŸ–¼ä½¿ç”¨å›¾åƒè½¬æ–‡æœ¬æ¨¡å‹: {dialog.llm_id}")
    else:
        llm_model_config = TenantLLMService.get_model_config(dialog.tenant_id, LLMType.CHAT, dialog.llm_id)
        logging.info(f"ä½¿ç”¨èŠå¤©æ¨¡å‹: {dialog.llm_id}")

    max_tokens = llm_model_config.get("max_tokens", 8192)  # è·å–æ¨¡å‹æœ€å¤§tokenæ•°é™åˆ¶ï¼Œé»˜è®¤8192
    logging.info(f" æœ€å¤§tokené™åˆ¶: {max_tokens}")
    logging.info("[é˜¶æ®µ2] å®Œæˆ - æ¨¡å‹é…ç½®è·å–")


    check_llm_ts = timer()  # llmå¼€å§‹æ—¶é—´

    # ===== é˜¶æ®µ3: Langfuseç›‘æ§åˆå§‹åŒ– =====
    logging.info("[é˜¶æ®µ3] å¼€å§‹ - Langfuseç›‘æ§åˆå§‹åŒ–")
    # åˆå§‹åŒ–Langfuseè¿½è¸ªå™¨ï¼ˆç”¨äºç›‘æ§å’Œåˆ†æï¼‰
    langfuse_tracer = None
    # ä»æ•°æ®åº“æŸ¥è¯¢Langfuseå¯†é’¥, æˆ‘ä»¬ç›®å‰tenant_langfuseè¡¨æ— æ•°æ®ï¼Œæ‰€ä»¥langfuse_keysä¸ºç©ºã€‚
    langfuse_keys = TenantLangfuseService.filter_by_tenant(tenant_id=dialog.tenant_id)
    if langfuse_keys:
        print("Langfuse keys found")
        langfuse = Langfuse(public_key=langfuse_keys.public_key, secret_key=langfuse_keys.secret_key,
                            host=langfuse_keys.host)
        if langfuse.auth_check():
            langfuse_tracer = langfuse
            langfuse.trace = langfuse_tracer.trace(name=f"{dialog.name}-{llm_model_config['llm_name']}")

    check_langfuse_tracer_ts = timer()

    # ===== é˜¶æ®µ4: æ¨¡å‹ç»‘å®š =====
    logging.info(" [é˜¶æ®µ4] å¼€å§‹ - æ¨¡å‹ç»‘å®š")
    # æ ¹æ®æ™ºèƒ½åŠ©ç†ï¼ˆdialogï¼‰é…ç½®ï¼Œè·å–æ¨¡å‹å®ä¾‹
    # - kbs: çŸ¥è¯†åº“å¯¹è±¡åˆ—è¡¨ï¼Œembd_mdl: åµŒå…¥æ¨¡å‹ï¼Œ rerank_mdl: é‡æ’åºæ¨¡å‹ï¼Œchat_mdl: èŠå¤©æ¨¡å‹ï¼Œtts_mdl: è¯­éŸ³åˆæˆæ¨¡å‹
    kbs, embd_mdl, rerank_mdl, chat_mdl, tts_mdl = get_models(dialog)
    # è·å–å·¥å…·è°ƒç”¨ä¼šè¯å’Œå·¥å…·åˆ—è¡¨
    toolcall_session, tools = kwargs.get("toolcall_session"), kwargs.get("tools")
    if toolcall_session and tools:
        # å¦‚æœæœ‰å·¥å…·ï¼Œåˆ™ç»‘å®šåˆ°èŠå¤©æ¨¡å‹
        logging.info(f"ğŸ”§ ç»‘å®šå·¥å…·æ•°é‡: {len(tools)}")
        chat_mdl.bind_tools(toolcall_session, tools)
    bind_models_ts = timer()

    # ===== é˜¶æ®µ5: é—®é¢˜é¢„å¤„ç† =====
    logging.info(" [é˜¶æ®µ5] å¼€å§‹ - é—®é¢˜é¢„å¤„ç†")
    # # è·å–æ£€ç´¢å™¨å®ä¾‹(å³å¬å›ç³»ç»Ÿ)
    retriever = settings.retrievaler
    # æå–æœ€è¿‘3æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºé—®é¢˜åˆ—è¡¨
    questions = [m["content"] for m in messages if m["role"] == "user"][-3:]
    logging.info(f"æå–ç”¨æˆ·é—®é¢˜æ•°é‡: {len(questions)}")
    logging.info(f"æœ€æ–°é—®é¢˜: {questions[-1][:100]}...")
    # å¤„ç†æ–‡æ¡£é™„ä»¶ID
    attachments = kwargs["doc_ids"].split(",") if "doc_ids" in kwargs else None
    if "doc_ids" in messages[-1]:
        attachments = messages[-1]["doc_ids"]

    # è·å–æç¤ºé…ç½®å’Œå­—æ®µæ˜ å°„
    prompt_config = dialog.prompt_config
    field_map = KnowledgebaseService.get_field_map(dialog.kb_ids)
    # å°è¯•ä½¿ç”¨SQLæ£€ç´¢ï¼ˆå¦‚æœå­—æ®µæ˜ å°„å¯ç”¨ï¼‰
    if field_map:
        logging.debug("Use SQL to retrieval:{}".format(questions[-1]))  # ä½¿ç”¨SQLæŸ¥è¯¢è·å–ç­”æ¡ˆ
        ans = use_sql(questions[-1], field_map, dialog.tenant_id, chat_mdl, prompt_config.get("quote", True))
        if ans:
            yield ans
            return

    # æ£€æŸ¥å¿…éœ€å‚æ•°æ˜¯å¦å­˜åœ¨
    for p in prompt_config["parameters"]:
        if p["key"] == "knowledge":
            continue
        # æ£€æŸ¥éoptionalå‚æ•°æ˜¯å¦é½å¤‡
        if p["key"] not in kwargs and not p["optional"]:
            raise KeyError("Miss parameter: " + p["key"])
        # å¦‚æœå¯é€‰å‚æ•°(optional)ç¼ºå¤±ï¼Œç”¨ç©ºæ ¼æ›¿æ¢å ä½ç¬¦
        if p["key"] not in kwargs:
            prompt_config["system"] = prompt_config["system"].replace("{%s}" % p["key"], " ")

    # ===== é˜¶æ®µ6: é—®é¢˜ç²¾åŒ– =====
    logging.info(" [é˜¶æ®µ6] å¼€å§‹ - é—®é¢˜ç²¾åŒ–")
    #  å¤„ç†å¤šè½®å¯¹è¯çš„é—®é¢˜ç²¾åŒ–
    if len(questions) > 1 and prompt_config.get("refine_multiturn"):
        # å¤šè½®å¯¹è¯æ¨¡å¼ï¼šå°†å†å²å¯¹è¯ä¸Šä¸‹æ–‡åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´ã€ç‹¬ç«‹çš„é—®é¢˜
        # ä¾‹å¦‚ï¼šç”¨æˆ·é—®"ç‰¹æœ—æ™®çš„çˆ¶äº²å«ä»€ä¹ˆ?" -> "Fred Trump" -> "ä»–çš„æ¯äº²å‘¢?" ä¼šè¢«ç²¾åŒ–ä¸ºï¼š"ç‰¹æœ—æ™®çš„æ¯äº²å«ä»€ä¹ˆåå­—?"
        questions = [full_question(dialog.tenant_id, dialog.llm_id, messages)]
    else:
        # å•è½®å¯¹è¯æ¨¡å¼ï¼šåªä½¿ç”¨ç”¨æˆ·çš„æœ€æ–°é—®é¢˜ï¼Œå¿½ç•¥å†å²ä¸Šä¸‹æ–‡
        questions = questions[-1:]

    # è·¨è¯­è¨€æŸ¥è¯¢æ‰©å±•
    if prompt_config.get("cross_languages"):
        # å°†é—®é¢˜ç¿»è¯‘æˆå¤šç§è¯­è¨€ä»¥æå‡æ£€ç´¢è¦†ç›–ç‡
        # ä¾‹å¦‚ï¼šè‹±æ–‡é—®é¢˜åŒæ—¶ç”Ÿæˆä¸­æ–‡ã€æ—¥æ–‡ç‰ˆæœ¬è¿›è¡Œæ£€ç´¢, è¿™æ ·å¯ä»¥æ£€ç´¢åˆ°ä¸åŒè¯­è¨€çš„ç›¸å…³æ–‡æ¡£
        questions = [cross_languages(dialog.tenant_id, dialog.llm_id, questions[0], prompt_config["cross_languages"])]

    # å…³é”®è¯æå–å¢å¼ºæŸ¥è¯¢
    if prompt_config.get("keyword", False):
        # ä»ç”¨æˆ·é—®é¢˜ä¸­æå–å…³é”®è¯å¹¶è¿½åŠ åˆ°åŸé—®é¢˜å
        # ä¾‹å¦‚ï¼š"å¦‚ä½•ä½¿ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æ?" + "Python, æ•°æ®åˆ†æ, æœºå™¨å­¦ä¹ ", è¿™æ ·å¯ä»¥æå‡å‘é‡æ£€ç´¢çš„å¬å›ç‡
        questions[-1] += keyword_extraction(chat_mdl, questions[-1])

    logging.info(questions)
    refine_question_ts = timer()

    # ===== é˜¶æ®µ7: çŸ¥è¯†æ£€ç´¢ =====
    logging.info(" [é˜¶æ®µ7] å¼€å§‹ - çŸ¥è¯†æ£€ç´¢")
    # åˆå§‹åŒ–æ¨ç†æ€è€ƒè¿‡ç¨‹å’ŒçŸ¥è¯†åº“æ£€ç´¢ç»“æœ
    thought = ""  # å­˜å‚¨æ¨ç†æ¨¡å¼ä¸‹çš„æ€è€ƒè¿‡ç¨‹æ–‡æœ¬
    # çŸ¥è¯†åº“æ£€ç´¢ç»“æœçš„æ ‡å‡†åŒ–æ ¼å¼
    # total:æ£€ç´¢åˆ°çš„æ€»æ–‡æ¡£æ•° chunks:æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªç‰‡æ®µåŒ…å«å†…å®¹ã€å‘é‡ã€å…ƒæ•°æ®ç­‰
    # doc_aggs:æ–‡æ¡£èšåˆä¿¡æ¯ï¼ŒåŒ…å«æ–‡æ¡£çº§åˆ«çš„ç»Ÿè®¡å’Œå…ƒæ•°æ®
    kbinfos = {"total": 0, "chunks": [], "doc_aggs": []}
    # æ£€æŸ¥çŸ¥è¯†æ£€ç´¢é…ç½®ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡ŒçŸ¥è¯†åº“æ£€ç´¢
    if "knowledge" not in [p["key"] for p in prompt_config["parameters"]]:
        # å¦‚æœprompté…ç½®ä¸­æ²¡æœ‰"knowledge"å‚æ•°ï¼Œè¯´æ˜è¿™æ˜¯çº¯èŠå¤©æ¨¡å¼
        # ä¸éœ€è¦æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼Œç›´æ¥ä½¿ç”¨LLMçš„å†…ç½®çŸ¥è¯†å›ç­”
        knowledges = []
    else:
        # éœ€è¦è¿›è¡ŒçŸ¥è¯†æ£€ç´¢çš„æ¨¡å¼,è·å–æ‰€æœ‰ç›¸å…³çŸ¥è¯†åº“çš„ç§Ÿæˆ·IDï¼ˆå»é‡å¤„ç†ï¼‰
        tenant_ids = list(set([kb.tenant_id for kb in kbs]))
        knowledges = []  # åˆå§‹åŒ–çŸ¥è¯†å†…å®¹åˆ—è¡¨
        # æ™ºèƒ½æ¨ç†(reasoning)æ¨¡å¼ vs æ ‡å‡†æ£€ç´¢æ¨¡å¼çš„åˆ†æ”¯å¤„ç†
        if prompt_config.get("reasoning", False):
            # == = æ™ºèƒ½æ¨ç†æ¨¡å¼ == =
            # ä½¿ç”¨DeepResearcherè¿›è¡Œå¤šæ­¥éª¤æ¨ç†å’ŒçŸ¥è¯†æ£€ç´¢, è¿™ç§æ¨¡å¼ä¼šè¿›è¡Œæ›´æ·±å…¥çš„åˆ†æï¼Œç±»ä¼¼äºAIç ”ç©¶åŠ©æ‰‹
            reasoner = DeepResearcher(
                chat_mdl,  # èŠå¤©æ¨¡å‹ï¼Œç”¨äºæ¨ç†åˆ†æ
                prompt_config,  # æç¤ºé…ç½®
                # åˆ›å»ºæ£€ç´¢å‡½æ•°çš„åå‡½æ•°ï¼Œé¢„è®¾æ‰€æœ‰æ£€ç´¢å‚æ•°
                #   retriever.retrieval æ£€ç´¢å™¨çš„æ£€ç´¢æ–¹æ³•;  embd_mdl:åµŒå…¥æ¨¡å‹; tenant_ids:ç§Ÿæˆ·IDåˆ—è¡¨;kb_ids:çŸ¥è¯†åº“IDåˆ—è¡¨
                #   similarity_threshold:ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.2è¾ƒä½ï¼Œå¬å›æ›´å¤šï¼‰; vector_similarity_weight:å‘é‡ç›¸ä¼¼åº¦æƒé‡
                partial(retriever.retrieval, embd_mdl=embd_mdl, tenant_ids=tenant_ids, kb_ids=dialog.kb_ids, page=1,
                        page_size=dialog.top_n, similarity_threshold=0.2, vector_similarity_weight=0.3),
            )
            # æ‰§è¡Œæ¨ç†æ€è€ƒå¾ªç¯
            for think in reasoner.thinking(kbinfos, " ".join(questions)):
                # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯æœ€ç»ˆçš„æ€è€ƒç»“æœ
                if isinstance(think, str):
                    thought = think  # ä¿å­˜å®Œæ•´æ€è€ƒè¿‡ç¨‹
                    knowledges = [t for t in think.split("\n") if t]  # æŒ‰è¡Œåˆ†å‰²å¹¶è¿‡æ»¤ç©ºè¡Œ
                elif stream:
                    # å¦‚æœè¿”å›çš„æ˜¯æµå¼æ•°æ®ä¸”å¼€å¯äº†æµå¼è¾“å‡º,å®æ—¶å‘ç”¨æˆ·å±•ç¤ºæ¨ç†è¿‡ç¨‹ï¼ˆå¦‚"æ­£åœ¨åˆ†æ..."ï¼‰
                    yield think
        else:
            # === æ ‡å‡†æ£€ç´¢æ¨¡å¼ ===
            # ä¼ ç»Ÿçš„å‘é‡æ£€ç´¢ + å¯é€‰çš„å¤šæºæ£€ç´¢å¢å¼º
            # 1. å‘é‡æ£€ç´¢ï¼ˆæ ¸å¿ƒæ£€ç´¢ï¼‰
            if embd_mdl:
                # æ‰§è¡Œè¯­ä¹‰å‘é‡æ£€ç´¢(å‘é‡å¬å›)
                kbinfos = retriever.retrieval(
                    " ".join(questions),  # åˆå¹¶åçš„æŸ¥è¯¢æ–‡æœ¬
                    embd_mdl,  # åµŒå…¥æ¨¡å‹ï¼ˆå°†æ–‡æœ¬è½¬ä¸ºå‘é‡ï¼‰
                    tenant_ids,  # ç§Ÿæˆ·èŒƒå›´é™åˆ¶
                    dialog.kb_ids,  # çŸ¥è¯†åº“èŒƒå›´é™åˆ¶
                    1,  # åˆ†é¡µï¼šç¬¬1é¡µ
                    dialog.top_n,  # è¿”å›ç‰‡æ®µæ•°é‡ï¼ˆå¦‚50ä¸ªç‰‡æ®µï¼‰
                    dialog.similarity_threshold,  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¦‚0.7ï¼Œè¿‡æ»¤ä¸ç›¸å…³å†…å®¹ï¼‰
                    dialog.vector_similarity_weight,  # å‘é‡vså…³é”®è¯æƒé‡å¹³è¡¡ï¼ˆå¦‚0.7è¡¨ç¤º70%å‘é‡+30%å…³é”®è¯ï¼‰
                    doc_ids=attachments,  # å¦‚æœç”¨æˆ·ä¸Šä¼ äº†ç‰¹å®šæ–‡æ¡£ï¼Œä¼˜å…ˆæ£€ç´¢è¿™äº›æ–‡æ¡£
                    top=dialog.top_k,  # é‡æ’åºåä¿ç•™çš„æœ€ç»ˆç»“æœæ•°ï¼ˆå¦‚10ä¸ªï¼‰
                    aggs=False,  # ä¸è¿›è¡Œèšåˆç»Ÿè®¡
                    rerank_mdl=rerank_mdl,  # é‡æ’åºæ¨¡å‹ï¼ˆæå‡æ£€ç´¢ç²¾åº¦ï¼‰
                    rank_feature=label_question(" ".join(questions), kbs),  # é—®é¢˜åˆ†ç±»ç‰¹å¾ï¼Œè¾…åŠ©æ’åº
                )
            # 2. ç½‘ç»œæœç´¢å¢å¼ºï¼ˆå¯é€‰ï¼‰
            if prompt_config.get("tavily_api_key"):
                # ä½¿ç”¨Tavilyè¿›è¡Œå®æ—¶ç½‘ç»œæœç´¢ï¼Œè·å–æœ€æ–°ä¿¡æ¯, é€‚ç”¨äºéœ€è¦æœ€æ–°æ•°æ®çš„é—®é¢˜ï¼ˆå¦‚"ä»Šå¤©çš„è‚¡ä»·"ã€"æœ€æ–°æ–°é—»"ç­‰ï¼‰
                tav = Tavily(prompt_config["tavily_api_key"])
                tav_res = tav.retrieve_chunks(" ".join(questions))  # æ£€ç´¢ç½‘ç»œå†…å®¹å—
                # å°†ç½‘ç»œæœç´¢ç»“æœåˆå¹¶åˆ°çŸ¥è¯†åº“ç»“æœä¸­
                kbinfos["chunks"].extend(tav_res["chunks"])  # æ·»åŠ å†…å®¹ç‰‡æ®µ
                kbinfos["doc_aggs"].extend(tav_res["doc_aggs"])  # æ·»åŠ æ–‡æ¡£ç»Ÿè®¡

            # 3. çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºï¼ˆå¯é€‰ï¼‰
            if prompt_config.get("use_kg"):
                # ä½¿ç”¨çŸ¥è¯†å›¾è°±è¿›è¡Œç»“æ„åŒ–çŸ¥è¯†æ£€ç´¢, çŸ¥è¯†å›¾è°±é€‚åˆå¤„ç†å®ä½“å…³ç³»ã€æ¦‚å¿µå±‚æ¬¡ç­‰ç»“æ„åŒ–é—®é¢˜
                ck = settings.kg_retrievaler.retrieval(
                    " ".join(questions),  # æŸ¥è¯¢æ–‡æœ¬
                    tenant_ids,  # ç§Ÿæˆ·èŒƒå›´
                    dialog.kb_ids,  # çŸ¥è¯†åº“èŒƒå›´
                    embd_mdl,  # åµŒå…¥æ¨¡å‹
                    LLMBundle(dialog.tenant_id, LLMType.CHAT))  # LLMæ¨¡å‹æŸ
                if ck["content_with_weight"]:
                    # å¦‚æœçŸ¥è¯†å›¾è°±æ£€ç´¢åˆ°äº†åŠ æƒå†…å®¹ï¼Œæ’å…¥åˆ°ç»“æœæœ€å‰é¢
                    # çŸ¥è¯†å›¾è°±çš„ç»“æœé€šå¸¸æ›´å‡†ç¡®ï¼Œæ‰€ä»¥ä¼˜å…ˆçº§æ›´é«˜
                    kbinfos["chunks"].insert(0, ck)

            # 4. ç”Ÿæˆæœ€ç»ˆçš„çŸ¥è¯†æç¤ºæ–‡æœ¬
            # å°†æ£€ç´¢åˆ°çš„æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µæ ¼å¼åŒ–ä¸ºLLMå¯ç†è§£çš„æç¤ºæ–‡æœ¬
            # åŒæ—¶è€ƒè™‘tokené™åˆ¶ï¼Œé¿å…è¶…å‡ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦
            knowledges = kb_prompt(kbinfos, max_tokens)

    # è®°å½•è°ƒè¯•ä¿¡æ¯ï¼šé—®é¢˜å’Œå¯¹åº”çš„çŸ¥è¯†
    logging.debug("{}->{}".format(" ".join(questions), "\n->".join(knowledges)))

    # ===== é˜¶æ®µ8: æ¶ˆæ¯æ„å»º =====
    logging.info(" [é˜¶æ®µ8] å¼€å§‹ - æ¶ˆæ¯æ„å»º")
    retrieval_ts = timer()
    # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°çŸ¥è¯†ä¸”é…ç½®äº†ç©ºå“åº”
    if not knowledges and prompt_config.get("empty_response"):
        empty_res = prompt_config["empty_response"]
        # è¿”å›ç©ºå“åº”ç»“æœ
        yield {"answer": empty_res, "reference": kbinfos, "prompt": "\n\n### Query:\n%s" % " ".join(questions),
               "audio_binary": tts(tts_mdl, empty_res)}
        return {"answer": prompt_config["empty_response"], "reference": kbinfos}

    # å°†çŸ¥è¯†å†…å®¹æ·»åŠ åˆ°kwargsä¸­
    kwargs["knowledge"] = "\n------\n" + "\n\n------\n\n".join(knowledges)
    # è·å–ç”Ÿæˆé…ç½®
    gen_conf = dialog.llm_setting
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
    msg = [{"role": "system", "content": prompt_config["system"].format(**kwargs)}]
    # åˆå§‹åŒ–å¼•ç”¨æç¤º
    prompt4citation = ""
    # å¦‚æœæœ‰çŸ¥è¯†ä¸”éœ€è¦å¼•ç”¨
    if knowledges and (prompt_config.get("quote", True) and kwargs.get("quote", True)):
        prompt4citation = citation_prompt()
    # æ·»åŠ å†å²æ¶ˆæ¯ï¼Œç§»é™¤ç³»ç»Ÿæ¶ˆæ¯å’Œå¼•ç”¨æ ‡è®°
    msg.extend([{"role": m["role"], "content": re.sub(r"##\d+\$\$", "", m["content"])} for m in messages if
                m["role"] != "system"])
    # ç¡®ä¿æ¶ˆæ¯åœ¨tokené™åˆ¶å†…
    used_token_count, msg = message_fit_in(msg, int(max_tokens * 0.95))
    # ç¡®ä¿è‡³å°‘æœ‰ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
    assert len(msg) >= 2, f"message_fit_in has bug: {msg}"
    # è·å–æœ€ç»ˆçš„æç¤ºå†…å®¹
    prompt = msg[0]["content"]

    # è°ƒæ•´ç”Ÿæˆé…ç½®ä¸­çš„æœ€å¤§tokenæ•°
    if "max_tokens" in gen_conf:
        gen_conf["max_tokens"] = min(gen_conf["max_tokens"], max_tokens - used_token_count)

    # ===== é˜¶æ®µ9: ç­”æ¡ˆç”Ÿæˆ =====
    logging.info(" [é˜¶æ®µ9] å¼€å§‹ - ç­”æ¡ˆç”Ÿæˆ")
    def decorate_answer(answer):
        """
            è£…é¥°ç­”æ¡ˆï¼Œæ·»åŠ å¼•ç”¨ã€ç»Ÿè®¡ä¿¡æ¯ç­‰.
            è¿™ä¸ªå‡½æ•°å®é™…ä¸Šæ˜¯ä¸€ä¸ª**"åå¤„ç†+æŠ¥å‘Šç”Ÿæˆå™¨"**ï¼Œä¸»è¦åšä¸‰ä»¶äº‹ï¼š
            1. è£…é¥°ç­”æ¡ˆå†…å®¹ã€‚ï¼ˆå¤„ç†å¼•ç”¨ã€ä¿®å¤å¼•ç”¨æ ¼å¼ï¼‰
            2. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šã€‚ï¼ˆè®¡ç®—å„é˜¶æ®µè€—æ—¶ã€æ„å»ºæŠ¥å‘Šæ–‡æœ¬ prompt += ...  æ³¨æ„è¿™ä¸ªpromptæ˜¯ç»™å±•ç¤ºç»™ç”¨æˆ·çš„è°ƒè¯•ä¿¡æ¯ï¼Œä¸æ˜¯ç»™LLMçš„æç¤ºï¼‰
            3. è¿”å›å®Œæ•´çš„è°ƒè¯•ä¿¡æ¯
        """
        nonlocal embd_mdl, prompt_config, knowledges, kwargs, kbinfos, prompt, retrieval_ts, questions, langfuse_tracer
        # åˆå§‹åŒ–å¼•ç”¨åˆ—è¡¨
        refs = []
        # åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ
        ans = answer.split("</think>")
        think = ""
        if len(ans) == 2:
            think = ans[0] + "</think>"
            answer = ans[1]

        # å¤„ç†å¼•ç”¨å’Œå‚è€ƒæ–‡çŒ®
        if knowledges and (prompt_config.get("quote", True) and kwargs.get("quote", True)):
            idx = set([])
            # å¦‚æœæ²¡æœ‰ç°æœ‰å¼•ç”¨ï¼Œè‡ªåŠ¨æ’å…¥å¼•ç”¨
            if embd_mdl and not re.search(r"\[ID:([0-9]+)\]", answer):
                answer, idx = retriever.insert_citations(
                    answer,
                    [ck["content_ltks"] for ck in kbinfos["chunks"]],  # å†…å®¹tokens
                    [ck["vector"] for ck in kbinfos["chunks"]],  # å‘é‡
                    embd_mdl,  # åµŒå…¥æ¨¡å‹
                    tkweight=1 - dialog.vector_similarity_weight,  # tokenæƒé‡
                    vtweight=dialog.vector_similarity_weight,  # å‘é‡æƒé‡
                )
            else:
                # æå–ç°æœ‰å¼•ç”¨ID
                for match in re.finditer(r"\[ID:([0-9]+)\]", answer):
                    i = int(match.group(1))
                    if i < len(kbinfos["chunks"]):
                        idx.add(i)
            # ä¿®å¤é”™è¯¯çš„å¼•ç”¨æ ¼å¼
            answer, idx = repair_bad_citation_formats(answer, kbinfos, idx)
            # è·å–è¢«å¼•ç”¨çš„æ–‡æ¡£ID
            idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
            # ç­›é€‰è¢«å¼•ç”¨çš„æ–‡æ¡£
            recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
            if not recall_docs:
                recall_docs = kbinfos["doc_aggs"]
            kbinfos["doc_aggs"] = recall_docs
            # åˆ›å»ºå¼•ç”¨ä¿¡æ¯çš„æ·±æ‹·è´
            refs = deepcopy(kbinfos)
            # ç§»é™¤å‘é‡ä¿¡æ¯ä»¥å‡å°‘è¾“å‡ºå¤§å°
            for c in refs["chunks"]:
                if c.get("vector"):
                    del c["vector"]
        # æ£€æŸ¥APIå¯†é’¥ç›¸å…³é”™è¯¯
        if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
            answer += " Please set LLM API-Key in 'User Setting -> Model providers -> API-Key'"
        finish_chat_ts = timer()

        # è®¡ç®—å„é˜¶æ®µè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        total_time_cost = (finish_chat_ts - chat_start_ts) * 1000
        check_llm_time_cost = (check_llm_ts - chat_start_ts) * 1000
        check_langfuse_tracer_cost = (check_langfuse_tracer_ts - check_llm_ts) * 1000
        bind_embedding_time_cost = (bind_models_ts - check_langfuse_tracer_ts) * 1000
        refine_question_time_cost = (refine_question_ts - bind_models_ts) * 1000
        retrieval_time_cost = (retrieval_ts - refine_question_ts) * 1000
        generate_result_time_cost = (finish_chat_ts - retrieval_ts) * 1000
        # è®¡ç®—ç”Ÿæˆçš„tokenæ•°é‡
        tk_num = num_tokens_from_string(think + answer)
        # æ„å»ºå®Œæ•´çš„æç¤ºä¿¡æ¯
        prompt += "\n\n### Query:\n%s" % " ".join(questions)
        prompt = (
            f"{prompt}\n\n"
            "## Time elapsed:\n"
            f"  - Total: {total_time_cost:.1f}ms\n"
            f"  - Check LLM: {check_llm_time_cost:.1f}ms\n"
            f"  - Check Langfuse tracer: {check_langfuse_tracer_cost:.1f}ms\n"
            f"  - Bind models: {bind_embedding_time_cost:.1f}ms\n"
            f"  - Query refinement(LLM): {refine_question_time_cost:.1f}ms\n"
            f"  - Retrieval: {retrieval_time_cost:.1f}ms\n"
            f"  - Generate answer: {generate_result_time_cost:.1f}ms\n\n"
            "## Token usage:\n"
            f"  - Generated tokens(approximately): {tk_num}\n"
            f"  - Token speed: {int(tk_num / (generate_result_time_cost / 1000.0))}/s"
        )
        # å‡†å¤‡Langfuseè¾“å‡ºä¿¡æ¯
        langfuse_output = "\n" + re.sub(r"^.*?(### Query:.*)", r"\1", prompt, flags=re.DOTALL)
        langfuse_output = {"time_elapsed:": re.sub(r"\n", "  \n", langfuse_output), "created_at": time.time()}

        # Add a condition check to call the end method only if langfuse_tracer exists
        # ç»“æŸLangfuseè¿½è¸ªï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if langfuse_tracer and "langfuse_generation" in locals():
            langfuse_generation.end(output=langfuse_output)
        # è¿”å›æœ€ç»ˆç»“æœ
        return {"answer": think + answer, "reference": refs, "prompt": re.sub(r"\n", "  \n", prompt),
                "created_at": time.time()}

    # å¼€å§‹Langfuseç”Ÿæˆè¿½è¸ª
    if langfuse_tracer:
        langfuse_generation = langfuse_tracer.trace.generation(name="chat", model=llm_model_config["llm_name"],
                                                               input={"prompt": prompt,
                                                                      "prompt4citation": prompt4citation,
                                                                      "messages": msg})
    # æ ¹æ®streamå‚æ•°é€‰æ‹©æµå¼æˆ–éæµå¼è¾“å‡º
    if stream:
        last_ans = ""
        answer = ""
        # è°ƒç”¨å¤§æ¨¡å‹ï¼Œæµå¼é€æ­¥ç”Ÿæˆç­”æ¡ˆ
        print("chat è°ƒç”¨å¤§æ¨¡å‹ï¼Œæµå¼é€æ­¥ç”Ÿæˆç­”æ¡ˆ")
        for ans in chat_mdl.chat_streamly(prompt + prompt4citation, msg[1:], gen_conf):
            if thought:
                # ç§»é™¤æ€è€ƒæ ‡ç­¾åçš„å†…å®¹
                ans = re.sub(r"^.*</think>", "", ans, flags=re.DOTALL)
            answer = ans
            # è®¡ç®—å¢é‡ç­”æ¡ˆ
            delta_ans = ans[len(last_ans):]
            # å¦‚æœå¢é‡å¤ªå°ï¼Œè·³è¿‡è¾“å‡º
            if num_tokens_from_string(delta_ans) < 16:
                continue
            last_ans = answer
            # æµå¼è¾“å‡ºåŒ…å«TTSéŸ³é¢‘
            yield {"answer": thought + answer, "reference": {}, "audio_binary": tts(tts_mdl, delta_ans)}
        # è¾“å‡ºæœ€åçš„å¢é‡
        delta_ans = answer[len(last_ans):]
        if delta_ans:
            yield {"answer": thought + answer, "reference": {}, "audio_binary": tts(tts_mdl, delta_ans)}

        # è¾“å‡ºæœ€ç»ˆè£…é¥°åçš„ç­”æ¡ˆ
        yield decorate_answer(thought + answer)
    else:
        # è°ƒç”¨å¤§æ¨¡å‹ï¼Œéæµå¼è¾“å‡º
        print("è°ƒç”¨å¤§æ¨¡å‹ï¼Œéæµå¼è¾“å‡º")
        answer = chat_mdl.chat(prompt + prompt4citation, msg[1:], gen_conf)
        # è®°å½•ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯
        user_content = msg[-1].get("content", "[content not available]")
        logging.debug("User: {}|Assistant: {}".format(user_content, answer))
        # è£…é¥°ç­”æ¡ˆå¹¶æ·»åŠ TTSéŸ³é¢‘
        res = decorate_answer(answer)
        res["audio_binary"] = tts(tts_mdl, answer)
        yield res


def use_sql(question, field_map, tenant_id, chat_mdl, quota=True):
    sys_prompt = "You are a Database Administrator. You need to check the fields of the following tables based on the user's list of questions and write the SQL corresponding to the last question."
    user_prompt = """
Table name: {};
Table of database fields are as follows:
{}

Question are as follows:
{}
Please write the SQL, only SQL, without any other explanations or text.
""".format(index_name(tenant_id), "\n".join([f"{k}: {v}" for k, v in field_map.items()]), question)
    tried_times = 0

    def get_table():
        nonlocal sys_prompt, user_prompt, question, tried_times
        sql = chat_mdl.chat(sys_prompt, [{"role": "user", "content": user_prompt}], {"temperature": 0.06})
        sql = re.sub(r"^.*</think>", "", sql, flags=re.DOTALL)
        logging.debug(f"{question} ==> {user_prompt} get SQL: {sql}")
        sql = re.sub(r"[\r\n]+", " ", sql.lower())
        sql = re.sub(r".*select ", "select ", sql.lower())
        sql = re.sub(r" +", " ", sql)
        sql = re.sub(r"([;ï¼›]|```).*", "", sql)
        if sql[: len("select ")] != "select ":
            return None, None
        if not re.search(r"((sum|avg|max|min)\(|group by )", sql.lower()):
            if sql[: len("select *")] != "select *":
                sql = "select doc_id,docnm_kwd," + sql[6:]
            else:
                flds = []
                for k in field_map.keys():
                    if k in forbidden_select_fields4resume:
                        continue
                    if len(flds) > 11:
                        break
                    flds.append(k)
                sql = "select doc_id,docnm_kwd," + ",".join(flds) + sql[8:]

        logging.debug(f"{question} get SQL(refined): {sql}")
        tried_times += 1
        return settings.retrievaler.sql_retrieval(sql, format="json"), sql

    tbl, sql = get_table()
    if tbl is None:
        return None
    if tbl.get("error") and tried_times <= 2:
        user_prompt = """
        Table name: {};
        Table of database fields are as follows:
        {}

        Question are as follows:
        {}
        Please write the SQL, only SQL, without any other explanations or text.


        The SQL error you provided last time is as follows:
        {}

        Error issued by database as follows:
        {}

        Please correct the error and write SQL again, only SQL, without any other explanations or text.
        """.format(index_name(tenant_id), "\n".join([f"{k}: {v}" for k, v in field_map.items()]), question, sql,
                   tbl["error"])
        tbl, sql = get_table()
        logging.debug("TRY it again: {}".format(sql))

    logging.debug("GET table: {}".format(tbl))
    if tbl.get("error") or len(tbl["rows"]) == 0:
        return None

    docid_idx = set([ii for ii, c in enumerate(tbl["columns"]) if c["name"] == "doc_id"])
    doc_name_idx = set([ii for ii, c in enumerate(tbl["columns"]) if c["name"] == "docnm_kwd"])
    column_idx = [ii for ii in range(len(tbl["columns"])) if ii not in (docid_idx | doc_name_idx)]

    # compose Markdown table
    columns = (
            "|" + "|".join(
        [re.sub(r"(/.*|ï¼ˆ[^ï¼ˆï¼‰]+ï¼‰)", "", field_map.get(tbl["columns"][i]["name"], tbl["columns"][i]["name"])) for i in
         column_idx]) + ("|Source|" if docid_idx and docid_idx else "|")
    )

    line = "|" + "|".join(["------" for _ in range(len(column_idx))]) + ("|------|" if docid_idx and docid_idx else "")

    rows = ["|" + "|".join([rmSpace(str(r[i])) for i in column_idx]).replace("None", " ") + "|" for r in tbl["rows"]]
    rows = [r for r in rows if re.sub(r"[ |]+", "", r)]
    if quota:
        rows = "\n".join([r + f" ##{ii}$$ |" for ii, r in enumerate(rows)])
    else:
        rows = "\n".join([r + f" ##{ii}$$ |" for ii, r in enumerate(rows)])
    rows = re.sub(r"T[0-9]{2}:[0-9]{2}:[0-9]{2}(\.[0-9]+Z)?\|", "|", rows)

    if not docid_idx or not doc_name_idx:
        logging.warning("SQL missing field: " + sql)
        return {"answer": "\n".join([columns, line, rows]), "reference": {"chunks": [], "doc_aggs": []},
                "prompt": sys_prompt}

    docid_idx = list(docid_idx)[0]
    doc_name_idx = list(doc_name_idx)[0]
    doc_aggs = {}
    for r in tbl["rows"]:
        if r[docid_idx] not in doc_aggs:
            doc_aggs[r[docid_idx]] = {"doc_name": r[doc_name_idx], "count": 0}
        doc_aggs[r[docid_idx]]["count"] += 1
    return {
        "answer": "\n".join([columns, line, rows]),
        "reference": {
            "chunks": [{"doc_id": r[docid_idx], "docnm_kwd": r[doc_name_idx]} for r in tbl["rows"]],
            "doc_aggs": [{"doc_id": did, "doc_name": d["doc_name"], "count": d["count"]} for did, d in
                         doc_aggs.items()],
        },
        "prompt": sys_prompt,
    }


def tts(tts_mdl, text):
    if not tts_mdl or not text:
        return
    bin = b""
    for chunk in tts_mdl.tts(text):
        bin += chunk
    return binascii.hexlify(bin).decode("utf-8")


def ask(question, kb_ids, tenant_id):
    print("======asking...")
    kbs = KnowledgebaseService.get_by_ids(kb_ids)
    embedding_list = list(set([kb.embd_id for kb in kbs]))

    is_knowledge_graph = all([kb.parser_id == ParserType.KG for kb in kbs])
    retriever = settings.retrievaler if not is_knowledge_graph else settings.kg_retrievaler

    embd_mdl = LLMBundle(tenant_id, LLMType.EMBEDDING, embedding_list[0])
    chat_mdl = LLMBundle(tenant_id, LLMType.CHAT)
    max_tokens = chat_mdl.max_length
    tenant_ids = list(set([kb.tenant_id for kb in kbs]))
    kbinfos = retriever.retrieval(question, embd_mdl, tenant_ids, kb_ids, 1, 12, 0.1, 0.3, aggs=False,
                                  rank_feature=label_question(question, kbs))
    knowledges = kb_prompt(kbinfos, max_tokens)
    prompt = """
    Role: You're a smart assistant. Your name is Miss R.
    Task: Summarize the information from knowledge bases and answer user's question.
    Requirements and restriction:
      - DO NOT make things up, especially for numbers.
      - If the information from knowledge is irrelevant with user's question, JUST SAY: Sorry, no relevant information provided.
      - Answer with markdown format text.
      - Answer in language of user's question.
      - DO NOT make things up, especially for numbers.

    ### Information from knowledge bases
    %s

    The above is information from knowledge bases.

    """ % "\n".join(knowledges)
    msg = [{"role": "user", "content": question}]

    def decorate_answer(answer):
        nonlocal knowledges, kbinfos, prompt
        answer, idx = retriever.insert_citations(answer, [ck["content_ltks"] for ck in kbinfos["chunks"]],
                                                 [ck["vector"] for ck in kbinfos["chunks"]], embd_mdl, tkweight=0.7,
                                                 vtweight=0.3)
        idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
        recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
        if not recall_docs:
            recall_docs = kbinfos["doc_aggs"]
        kbinfos["doc_aggs"] = recall_docs
        refs = deepcopy(kbinfos)
        for c in refs["chunks"]:
            if c.get("vector"):
                del c["vector"]

        if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
            answer += " Please set LLM API-Key in 'User Setting -> Model Providers -> API-Key'"
        refs["chunks"] = chunks_format(refs)
        return {"answer": answer, "reference": refs}

    answer = ""
    print("ask è°ƒç”¨å¤§æ¨¡å‹ï¼Œæµå¼é€æ­¥ç”Ÿæˆç­”æ¡ˆ")
    for ans in chat_mdl.chat_streamly(prompt, msg, {"temperature": 0.1}):
        answer = ans
        yield {"answer": answer, "reference": {}}
    yield decorate_answer(answer)
