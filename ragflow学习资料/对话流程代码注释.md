
​    

```python
def chat(dialog, messages, stream=True, **kwargs):
    """
    实现完整的RAG对话流程
Args:
    dialog: 对话实例，包含配置信息
    messages: 消息历史列表
    stream: 是否流式输出
    **kwargs: 其他参数
"""
print("实现完整的RAG对话流程")

# 验证最后一条消息必须来自用户
assert messages[-1]["role"] == "user", "The last content of this conversation is not from user."

# 如果没有知识库且没有Tavily API密钥，则使用简单对话模式
if not dialog.kb_ids and not dialog.prompt_config.get("tavily_api_key"):
    for ans in chat_solo(dialog, messages, stream):
        yield ans
    return

# 记录对话开始时间戳
chat_start_ts = timer()

# 根据LLM类型获取模型配置
if llm_id2llm_type(dialog.llm_id) == "image2text":
    llm_model_config = TenantLLMService.get_model_config(dialog.tenant_id, LLMType.IMAGE2TEXT, dialog.llm_id)
else:
    llm_model_config = TenantLLMService.get_model_config(dialog.tenant_id, LLMType.CHAT, dialog.llm_id)

# 获取模型最大token数限制，默认8192
max_tokens = llm_model_config.get("max_tokens", 8192)

# 记录检查LLM配置的时间戳
check_llm_ts = timer()

# 初始化Langfuse追踪器（用于监控和分析）
langfuse_tracer = None
# 获取租户的Langfuse配置
langfuse_keys = TenantLangfuseService.filter_by_tenant(tenant_id=dialog.tenant_id)
if langfuse_keys:
    # 创建Langfuse实例
    langfuse = Langfuse(public_key=langfuse_keys.public_key, secret_key=langfuse_keys.secret_key, host=langfuse_keys.host)
    # 验证认证并初始化追踪
    if langfuse.auth_check():
        langfuse_tracer = langfuse
        langfuse.trace = langfuse_tracer.trace(name=f"{dialog.name}-{llm_model_config['llm_name']}")

# 记录检查Langfuse追踪器的时间戳
check_langfuse_tracer_ts = timer()

# 根据智能助理（dialog）配置，获取所需的各种模型实例
# kbs: 知识库对象列表
# embd_mdl: 嵌入模型（用于向量化文本）
# rerank_mdl: 重排序模型（用于结果重新排序）
# chat_mdl: 聊天模型（用于生成回复）
# tts_mdl: 语音合成模型（用于文本转语音）
kbs, embd_mdl, rerank_mdl, chat_mdl, tts_mdl = get_models(dialog)

# 获取工具调用会话和工具列表
toolcall_session, tools = kwargs.get("toolcall_session"), kwargs.get("tools")
# 如果有工具，则绑定到聊天模型
if toolcall_session and tools:
    chat_mdl.bind_tools(toolcall_session, tools)

# 记录绑定模型的时间戳
bind_models_ts = timer()

# 获取检索器实例
retriever = settings.retrievaler

# 提取最近3条用户消息作为问题列表
questions = [m["content"] for m in messages if m["role"] == "user"][-3:]

# 处理文档附件ID
attachments = kwargs["doc_ids"].split(",") if "doc_ids" in kwargs else None
if "doc_ids" in messages[-1]:
    attachments = messages[-1]["doc_ids"]

# 获取提示配置和字段映射
prompt_config = dialog.prompt_config
field_map = KnowledgebaseService.get_field_map(dialog.kb_ids)

# 尝试使用SQL检索（如果字段映射可用）
if field_map:
    logging.debug("Use SQL to retrieval:{}".format(questions[-1]))
    # 使用SQL查询获取答案
    ans = use_sql(questions[-1], field_map, dialog.tenant_id, chat_mdl, prompt_config.get("quote", True))
    if ans:
        yield ans
        return

# 检查必需参数是否存在
for p in prompt_config["parameters"]:
    # 跳过knowledge参数
    if p["key"] == "knowledge":
        continue
    # 检查非可选参数是否缺失
    if p["key"] not in kwargs and not p["optional"]:
        raise KeyError("Miss parameter: " + p["key"])
    # 如果可选参数缺失，用空格替换占位符
    if p["key"] not in kwargs:
        prompt_config["system"] = prompt_config["system"].replace("{%s}" % p["key"], " ")

# 处理多轮对话的问题精化
if len(questions) > 1 and prompt_config.get("refine_multiturn"):
    # 多轮对话模式：将历史对话上下文合并为一个完整、独立的问题
    # 例如：用户问"特朗普的父亲叫什么?" -> "Fred Trump" -> "他的母亲呢?"
    # 会被精化为："特朗普的母亲叫什么名字?"
    questions = [full_question(dialog.tenant_id, dialog.llm_id, messages)]
else:
    # 单轮对话模式：只使用用户的最新问题，忽略历史上下文
    questions = questions[-1:]

# 跨语言查询扩展
if prompt_config.get("cross_languages"):
    # 将问题翻译成多种语言以提升检索覆盖率
    # 例如：英文问题同时生成中文、日文版本进行检索
    # 这样可以检索到不同语言的相关文档
    questions = [cross_languages(dialog.tenant_id, dialog.llm_id, questions[0], prompt_config["cross_languages"])]

# 关键词提取增强查询
if prompt_config.get("keyword", False):
    # 从用户问题中提取关键词并追加到原问题后
    # 例如："如何使用Python进行数据分析?" + "Python, 数据分析, 机器学习"
    # 这样可以提升向量检索的召回率
    questions[-1] += keyword_extraction(chat_mdl, questions[-1])

# 记录问题精化完成的时间戳
refine_question_ts = timer()

# 初始化推理思考过程和知识库检索结果
thought = ""  # 存储推理模式下的思考过程文本
kbinfos = {   # 知识库检索结果的标准化格式
    "total": 0,      # 检索到的总文档数
    "chunks": [],    # 文档片段列表，每个片段包含内容、向量、元数据等
    "doc_aggs": []   # 文档聚合信息，包含文档级别的统计和元数据
}

# 检查知识检索配置：判断是否需要进行知识库检索
if "knowledge" not in [p["key"] for p in prompt_config["parameters"]]:
    # 如果prompt配置中没有"knowledge"参数，说明这是纯聊天模式
    # 不需要检索外部知识，直接使用LLM的内置知识回答
    knowledges = []
else:
    # 需要进行知识检索的模式
    # 获取所有相关知识库的租户ID（去重处理）
    tenant_ids = list(set([kb.tenant_id for kb in kbs]))
    knowledges = []  # 初始化知识内容列表
    
    # 智能推理模式 vs 标准检索模式的分支处理
    if prompt_config.get("reasoning", False):
        # === 智能推理模式 ===
        # 使用DeepResearcher进行多步骤推理和知识检索
        # 这种模式会进行更深入的分析，类似于AI研究助手
        reasoner = DeepResearcher(
            chat_mdl,                    # 聊天模型，用于推理分析
            prompt_config,               # 提示配置
            # 创建检索函数的偏函数，预设所有检索参数
            partial(
                retriever.retrieval,     # 检索器的检索方法
                embd_mdl=embd_mdl,       # 嵌入模型
                tenant_ids=tenant_ids,   # 租户ID列表
                kb_ids=dialog.kb_ids,    # 知识库ID列表
                page=1,                  # 分页：第1页
                page_size=dialog.top_n,  # 每页大小
                similarity_threshold=0.2, # 相似度阈值（较低，召回更多）
                vector_similarity_weight=0.3  # 向量相似度权重
            ),
        )

        # 执行推理思考循环
        for think in reasoner.thinking(kbinfos, " ".join(questions)):
            if isinstance(think, str):
                # 如果返回的是字符串，说明是最终的思考结果
                thought = think                              # 保存完整思考过程
                knowledges = [t for t in think.split("\n") if t]  # 按行分割并过滤空行
            elif stream:
                # 如果返回的是流式数据且开启了流式输出
                # 实时向用户展示推理过程（如"正在分析..."、"查找相关文档..."）
                yield think
    else:
        # === 标准检索模式 ===
        # 传统的向量检索 + 可选的多源检索增强
        
        # 1. 向量检索（核心检索）
        if embd_mdl:
            # 执行语义向量检索
            kbinfos = retriever.retrieval(
                " ".join(questions),              # 合并后的查询文本
                embd_mdl,                         # 嵌入模型（将文本转为向量）
                tenant_ids,                       # 租户范围限制
                dialog.kb_ids,                    # 知识库范围限制
                1,                                # 分页：第1页
                dialog.top_n,                     # 返回片段数量（如50个片段）
                dialog.similarity_threshold,       # 相似度阈值（如0.7，过滤不相关内容）
                dialog.vector_similarity_weight,   # 向量vs关键词权重平衡（如0.7表示70%向量+30%关键词）
                doc_ids=attachments,              # 如果用户上传了特定文档，优先检索这些文档
                top=dialog.top_k,                 # 重排序后保留的最终结果数（如10个）
                aggs=False,                       # 不进行聚合统计
                rerank_mdl=rerank_mdl,            # 重排序模型（提升检索精度）
                rank_feature=label_question(" ".join(questions), kbs), # 问题分类特征，辅助排序
            )
        
        # 2. 网络搜索增强（可选）
        if prompt_config.get("tavily_api_key"):
            # 使用Tavily进行实时网络搜索，获取最新信息
            # 适用于需要最新数据的问题（如"今天的股价"、"最新新闻"等）
            tav = Tavily(prompt_config["tavily_api_key"])
            tav_res = tav.retrieve_chunks(" ".join(questions))  # 检索网络内容块
            # 将网络搜索结果合并到知识库结果中
            kbinfos["chunks"].extend(tav_res["chunks"])      # 添加内容片段
            kbinfos["doc_aggs"].extend(tav_res["doc_aggs"])  # 添加文档统计
        
        # 3. 知识图谱检索增强（可选）
        if prompt_config.get("use_kg"):
            # 使用知识图谱进行结构化知识检索
            # 知识图谱适合处理实体关系、概念层次等结构化问题
            ck = settings.kg_retrievaler.retrieval(
                " ".join(questions),                              # 查询文本
                tenant_ids,                                       # 租户范围
                dialog.kb_ids,                                    # 知识库范围
                embd_mdl,                                         # 嵌入模型
                LLMBundle(dialog.tenant_id, LLMType.CHAT)        # LLM模型束
            )
            if ck["content_with_weight"]:
                # 如果知识图谱检索到了加权内容，插入到结果最前面
                # 知识图谱的结果通常更准确，所以优先级更高
                kbinfos["chunks"].insert(0, ck)

        # 4. 生成最终的知识提示文本
        # 将检索到的所有知识片段格式化为LLM可理解的提示文本
        # 同时考虑token限制，避免超出模型的上下文长度
        knowledges = kb_prompt(kbinfos, max_tokens)

# 记录调试信息：问题和对应的知识
logging.debug("{}->{}".format(" ".join(questions), "\n->".join(knowledges)))

# 记录检索完成的时间戳
retrieval_ts = timer()

# 如果没有检索到知识且配置了空响应
if not knowledges and prompt_config.get("empty_response"):
    empty_res = prompt_config["empty_response"]
    # 返回空响应结果
    yield {"answer": empty_res, "reference": kbinfos, "prompt": "\n\n### Query:\n%s" % " ".join(questions), "audio_binary": tts(tts_mdl, empty_res)}
    return {"answer": prompt_config["empty_response"], "reference": kbinfos}

# 将知识内容添加到kwargs中
kwargs["knowledge"] = "\n------\n" + "\n\n------\n\n".join(knowledges)

# 获取生成配置
gen_conf = dialog.llm_setting

# 构建消息列表，添加系统提示
msg = [{"role": "system", "content": prompt_config["system"].format(**kwargs)}]

# 初始化引用提示
prompt4citation = ""

# 如果有知识且需要引用
if knowledges and (prompt_config.get("quote", True) and kwargs.get("quote", True)):
    prompt4citation = citation_prompt()

# 添加历史消息，移除系统消息和引用标记
msg.extend([{"role": m["role"], "content": re.sub(r"##\d+\$\$", "", m["content"])} for m in messages if m["role"] != "system"])

# 确保消息在token限制内
used_token_count, msg = message_fit_in(msg, int(max_tokens * 0.95))

# 确保至少有系统消息和用户消息
assert len(msg) >= 2, f"message_fit_in has bug: {msg}"

# 获取最终的提示内容
prompt = msg[0]["content"]

# 调整生成配置中的最大token数
if "max_tokens" in gen_conf:
    gen_conf["max_tokens"] = min(gen_conf["max_tokens"], max_tokens - used_token_count)

def decorate_answer(answer):
    """
    装饰答案，添加引用、统计信息等
    """
    nonlocal embd_mdl, prompt_config, knowledges, kwargs, kbinfos, prompt, retrieval_ts, questions, langfuse_tracer

    # 初始化引用列表
    refs = []
    
    # 分离思考过程和答案
    ans = answer.split("</think>")
    think = ""
    if len(ans) == 2:
        think = ans[0] + "</think>"
        answer = ans[1]

    # 处理引用和参考文献
    if knowledges and (prompt_config.get("quote", True) and kwargs.get("quote", True)):
        idx = set([])
        
        # 如果没有现有引用，自动插入引用
        if embd_mdl and not re.search(r"\[ID:([0-9]+)\]", answer):
            answer, idx = retriever.insert_citations(
                answer,
                [ck["content_ltks"] for ck in kbinfos["chunks"]],  # 内容tokens
                [ck["vector"] for ck in kbinfos["chunks"]],        # 向量
                embd_mdl,                                          # 嵌入模型
                tkweight=1 - dialog.vector_similarity_weight,      # token权重
                vtweight=dialog.vector_similarity_weight,          # 向量权重
            )
        else:
            # 提取现有引用ID
            for match in re.finditer(r"\[ID:([0-9]+)\]", answer):
                i = int(match.group(1))
                if i < len(kbinfos["chunks"]):
                    idx.add(i)

        # 修复错误的引用格式
        answer, idx = repair_bad_citation_formats(answer, kbinfos, idx)

        # 获取被引用的文档ID
        idx = set([kbinfos["chunks"][int(i)]["doc_id"] for i in idx])
        
        # 筛选被引用的文档
        recall_docs = [d for d in kbinfos["doc_aggs"] if d["doc_id"] in idx]
        if not recall_docs:
            recall_docs = kbinfos["doc_aggs"]
        kbinfos["doc_aggs"] = recall_docs

        # 创建引用信息的深拷贝
        refs = deepcopy(kbinfos)
        # 移除向量信息以减少输出大小
        for c in refs["chunks"]:
            if c.get("vector"):
                del c["vector"]

    # 检查API密钥相关错误
    if answer.lower().find("invalid key") >= 0 or answer.lower().find("invalid api") >= 0:
        answer += " Please set LLM API-Key in 'User Setting -> Model providers -> API-Key'"
    
    # 记录完成时间
    finish_chat_ts = timer()

    # 计算各阶段耗时（毫秒）
    total_time_cost = (finish_chat_ts - chat_start_ts) * 1000
    check_llm_time_cost = (check_llm_ts - chat_start_ts) * 1000
    check_langfuse_tracer_cost = (check_langfuse_tracer_ts - check_llm_ts) * 1000
    bind_embedding_time_cost = (bind_models_ts - check_langfuse_tracer_ts) * 1000
    refine_question_time_cost = (refine_question_ts - bind_models_ts) * 1000
    retrieval_time_cost = (retrieval_ts - refine_question_ts) * 1000
    generate_result_time_cost = (finish_chat_ts - retrieval_ts) * 1000

    # 计算生成的token数量
    tk_num = num_tokens_from_string(think + answer)
    
    # 构建完整的提示信息
    prompt += "\n\n### Query:\n%s" % " ".join(questions)
    prompt = (
        f"{prompt}\n\n"
        "## Time elapsed:\n"
        f"  - Total: {total_time_cost:.1f}ms\n"
        f"  - Check LLM: {check_llm_time_cost:.1f}ms\n"
        f"  - Check Langfuse tracer: {check_langfuse_tracer_cost:.1f}ms\n"
        f"  - Bind models: {bind_embedding_time_cost:.1f}ms\n"
        f"  - Query refinement(LLM): {refine_question_time_cost:.1f}ms\n"
        f"  - Retrieval: {retrieval_time_cost:.1f}ms\n"
        f"  - Generate answer: {generate_result_time_cost:.1f}ms\n\n"
        "## Token usage:\n"
        f"  - Generated tokens(approximately): {tk_num}\n"
        f"  - Token speed: {int(tk_num / (generate_result_time_cost / 1000.0))}/s"
    )

    # 准备Langfuse输出信息
    langfuse_output = "\n" + re.sub(r"^.*?(### Query:.*)", r"\1", prompt, flags=re.DOTALL)
    langfuse_output = {"time_elapsed:": re.sub(r"\n", "  \n", langfuse_output), "created_at": time.time()}

    # 结束Langfuse追踪（如果存在）
    if langfuse_tracer and "langfuse_generation" in locals():
        langfuse_generation.end(output=langfuse_output)

    # 返回最终结果
    return {"answer": think + answer, "reference": refs, "prompt": re.sub(r"\n", "  \n", prompt), "created_at": time.time()}

# 开始Langfuse生成追踪
if langfuse_tracer:
    langfuse_generation = langfuse_tracer.trace.generation(name="chat", model=llm_model_config["llm_name"], input={"prompt": prompt, "prompt4citation": prompt4citation, "messages": msg})

# 根据stream参数选择流式或非流式输出
if stream:
    # 流式输出模式
    last_ans = ""
    answer = ""
    
    # 逐步生成答案
    for ans in chat_mdl.chat_streamly(prompt + prompt4citation, msg[1:], gen_conf):
        # 移除思考标签后的内容
        if thought:
            ans = re.sub(r"^.*</think>", "", ans, flags=re.DOTALL)
        answer = ans
        
        # 计算增量答案
        delta_ans = ans[len(last_ans) :]
        
        # 如果增量太小，跳过输出
        if num_tokens_from_string(delta_ans) < 16:
            continue
        
        last_ans = answer
        # 流式输出包含TTS音频
        yield {"answer": thought + answer, "reference": {}, "audio_binary": tts(tts_mdl, delta_ans)}
    
    # 输出最后的增量
    delta_ans = answer[len(last_ans) :]
    if delta_ans:
        yield {"answer": thought + answer, "reference": {}, "audio_binary": tts(tts_mdl, delta_ans)}
    
    # 输出最终装饰后的答案
    yield decorate_answer(thought + answer)
else:
    # 非流式输出模式
    answer = chat_mdl.chat(prompt + prompt4citation, msg[1:], gen_conf)
    
    # 记录用户和助手的对话
    user_content = msg[-1].get("content", "[content not available]")
    logging.debug("User: {}|Assistant: {}".format(user_content, answer))
    
    # 装饰答案并添加TTS音频
    res = decorate_answer(answer)
    res["audio_binary"] = tts(tts_mdl, answer)
    yield res
```



    